{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "## Outline\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"deepseek-ai/DeepSeek-V3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Changan\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi, Changan! Itâ€™s great to meet you. How can I assist you today? Whether you have a question, need some advice, or just want to chat, Iâ€™m here to help. Let me know whatâ€™s on your mind! ðŸ˜Š'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Changan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Changan\n",
      "AI: Hi, Changan! Itâ€™s great to meet you. How can I assist you today? Whether you have a question, need some advice, or just want to chat, Iâ€™m here to help. Let me know whatâ€™s on your mind! ðŸ˜Š\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ah, a classic question! The answer to 1 + 1 is **2**. Itâ€™s one of the first math problems many of us learn, and itâ€™s a great foundation for understanding addition. Let me know if youâ€™d like to explore more math concepts or anything elseâ€”Iâ€™m here to help! ðŸ˜Š'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Changan\n",
      "AI: Hi, Changan! Itâ€™s great to meet you. How can I assist you today? Whether you have a question, need some advice, or just want to chat, Iâ€™m here to help. Let me know whatâ€™s on your mind! ðŸ˜Š\n",
      "Human: What is 1+1?\n",
      "AI: Ah, a classic question! The answer to 1 + 1 is **2**. Itâ€™s one of the first math problems many of us learn, and itâ€™s a great foundation for understanding addition. Let me know if youâ€™d like to explore more math concepts or anything elseâ€”Iâ€™m here to help! ðŸ˜Š\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is **Changan**! You introduced yourself at the beginning of our conversation. Itâ€™s a unique and interesting nameâ€”does it have a special meaning or origin? Iâ€™d love to hear more about it if youâ€™d like to share! ðŸ˜Š'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Changan\n",
      "AI: Hi, Changan! Itâ€™s great to meet you. How can I assist you today? Whether you have a question, need some advice, or just want to chat, Iâ€™m here to help. Let me know whatâ€™s on your mind! ðŸ˜Š\n",
      "Human: What is 1+1?\n",
      "AI: Ah, a classic question! The answer to 1 + 1 is **2**. Itâ€™s one of the first math problems many of us learn, and itâ€™s a great foundation for understanding addition. Let me know if youâ€™d like to explore more math concepts or anything elseâ€”Iâ€™m here to help! ðŸ˜Š\n",
      "Human: What is my name?\n",
      "AI: Your name is **Changan**! You introduced yourself at the beginning of our conversation. Itâ€™s a unique and interesting nameâ€”does it have a special meaning or origin? Iâ€™d love to hear more about it if youâ€™d like to share! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi, my name is Changan\\nAI: Hi, Changan! Itâ€™s great to meet you. How can I assist you today? Whether you have a question, need some advice, or just want to chat, Iâ€™m here to help. Let me know whatâ€™s on your mind! ðŸ˜Š\\nHuman: What is 1+1?\\nAI: Ah, a classic question! The answer to 1 + 1 is **2**. Itâ€™s one of the first math problems many of us learn, and itâ€™s a great foundation for understanding addition. Let me know if youâ€™d like to explore more math concepts or anything elseâ€”Iâ€™m here to help! ðŸ˜Š\\nHuman: What is my name?\\nAI: Your name is **Changan**! You introduced yourself at the beginning of our conversation. Itâ€™s a unique and interesting nameâ€”does it have a special meaning or origin? Iâ€™d love to hear more about it if youâ€™d like to share! ðŸ˜Š'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory\n",
    "\n",
    "è®¾ç½®è®°å¿†çš„å¯¹è¯è½®æ¬¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, Changan! Itâ€™s great to meet you. How can I assist you today? Whether you have questions, need advice, or just want to chat, Iâ€™m here to help. Whatâ€™s on your mind? ðŸ˜Š'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Changan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, Changan! The answer to 1 + 1 is **2**. Itâ€™s one of the most basic and fundamental equations in mathematics. If you have any more questionsâ€”whether theyâ€™re simple or complexâ€”feel free to ask! ðŸ˜Š'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! I donâ€™t actually know your name unless you tell me. In our conversation, youâ€™ve been referred to as \"Changan,\" but if thatâ€™s not your name, feel free to let me know what I should call you! ðŸ˜Š How can I assist you today?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import BaseMessage  # ä¿®æ­£æ¶ˆæ¯ç±»å¯¼å…¥\n",
    "from langchain_openai import ChatOpenAI  # æ­£ç¡®å¯¼å…¥ChatOpenAI\n",
    "from langchain_openai.chat_models.base import BaseChatOpenAI\n",
    "\n",
    "# openai å¯¹äºŽå…¶ä»–æ¨¡åž‹æ²¡æœ‰get_num_tokens_from_messagesçš„å®žçŽ°æ‰€ä»¥åˆ›å»ºå­ç±»æ¥å®žçŽ°è¿™ä¸ªæ–¹æ³•\n",
    "class ChatOpenAIDeepSeek(ChatOpenAI):  # ç›´æŽ¥ç»§æ‰¿ChatOpenAI\n",
    "    def get_num_tokens_from_messages(self, messages: List[BaseMessage]) -> int:\n",
    "        model, encoding = self._get_encoding_model()\n",
    "        \n",
    "        if model.startswith(\"cl100k_base\"):\n",
    "            # ç›´æŽ¥è°ƒç”¨çˆ¶ç±»æ–¹æ³•è€Œéžç¥–çˆ¶ç±»\n",
    "            return super(BaseChatOpenAI, self).get_num_tokens_from_messages(messages)\n",
    "        else:\n",
    "            return super().get_num_tokens_from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAIDeepSeek(model=llm_model, temperature=0.0)\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human greets the AI, and they exchange casual small talk. The human mentions they are just hanging out, and the AI responds positively. The human then asks what is on the schedule for the day.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI, and they exchange casual small talk. The human mentions they are just hanging out, and the AI responds positively. The human then asks what is on the schedule for the day.\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A great demo to show would be one that highlights the capabilities of the latest Large Language Models (LLMs) in a way thatâ€™s both impressive and relevant to your customerâ€™s interests. Here are a few ideas:\\n\\n1. **Custom Chatbot with Domain-Specific Knowledge**:  \\n   - Show how you can fine-tune or use retrieval-augmented generation (RAG) with LangChain to create a chatbot that answers questions specific to the customerâ€™s industry. For example, if theyâ€™re in healthcare, you could demonstrate a chatbot that provides accurate medical information or assists with patient queries.\\n\\n2. **Automated Document Summarization**:  \\n   - Use LangChain to summarize a long document or report in seconds. This is especially useful for industries that deal with large volumes of text, like legal, finance, or research.\\n\\n3. **Interactive Data Analysis**:  \\n   - Demonstrate how an LLM can analyze and interpret data. For instance, you could upload a dataset (e.g., sales data) and ask the model to generate insights, trends, or even visualizations using LangChainâ€™s integration with tools like Pandas or Matplotlib.\\n\\n4. **Multi-Step Workflow Automation**:  \\n   - Showcase a complex workflow where the LLM interacts with APIs, databases, or other tools to complete a task. For example, you could simulate a customer service scenario where the model retrieves information from a CRM, drafts an email, and schedules a follow-up.\\n\\n5. **Creative Content Generation**:  \\n   - Highlight the modelâ€™s ability to generate creative content, such as marketing copy, blog posts, or even code snippets. You could ask the model to write a product description or generate a Python script for a specific task.\\n\\n6. **Real-Time Translation or Transcription**:  \\n   - If the customer deals with multilingual content, demonstrate real-time translation or transcription capabilities using LangChain and an LLM.\\n\\n7. **Personalized Recommendations**:  \\n   - Show how the model can provide personalized recommendations based on user input. For example, if the customer is in e-commerce, you could simulate a product recommendation engine.\\n\\nFor the demo, make sure to:  \\n- Keep it interactive and engaging.  \\n- Tailor the example to the customerâ€™s industry or needs.  \\n- Explain the underlying technology (e.g., LangChain, RAG, fine-tuning) in a way thatâ€™s accessible but still impressive.  \\n\\nLet me know if youâ€™d like help scripting or refining any of these ideas!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human greets the AI, and they exchange casual small talk. The human mentions they are just hanging out, and the AI responds positively. The human then asks what is on the schedule for the day. The AI outlines a busy day, including a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer to discuss the latest in AI. The human asks what would be a good demo to show, and the AI suggests several tailored ideas, such as a custom chatbot, automated document summarization, interactive data analysis, multi-step workflow automation, creative content generation, real-time translation, and personalized recommendations. The AI emphasizes the importance of making the demo interactive, relevant, and accessible while showcasing the capabilities of LangChain and LLMs.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
