{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains in LangChain\n",
    "\n",
    "## Outline\n",
    "\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "  * SimpleSequentialChain\n",
    "  * SequentialChain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"deepseek-ai/DeepSeek-V3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A descriptive and appealing name for a company that specializes in Queen Size Sheet Sets could emphasize comfort, quality, and elegance. Here are some suggestions:\\n\\n1. **Royal Rest Linens**  \\n2. **Queenly Comfort Co.**  \\n3. **Slumber Luxe**  \\n4. **Regal Threads**  \\n5. **Dream Haven Sheets**  \\n6. **Velvet Embrace Bedding**  \\n7. **Majestic Sleep Co.**  \\n8. **Elysian Linens**  \\n9. **Crown Comfort Sheets**  \\n10. **Serene Monarch Bedding**  \\n\\nThese names convey a sense of luxury and relaxation, which aligns well with premium bedding products. Choosing a name that resonates with your target audience and reflects your brand’s values is key!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 158, 'prompt_tokens': 20, 'total_tokens': 178, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-ad8c9c0c-ac58-4eae-9418-354445e81251-0', usage_metadata={'input_tokens': 20, 'output_tokens': 158, 'total_tokens': 178, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "chain.invoke(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = first_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = second_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接多个链\n",
    "combined_chain = chain_one | chain_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Luxurious queen-size sheet sets designed for ultimate comfort, elegance, and serene, restful nights.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 376, 'total_tokens': 398, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-8adb68fa-ef31-4b36-b280-09b50d4c783c-0', usage_metadata={'input_tokens': 376, 'output_tokens': 22, 'total_tokens': 398, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_chain.invoke(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = (\n",
    "    {\"Review\": RunnablePassthrough()}  # 传递输入\n",
    "    | first_prompt                    # 应用 prompt\n",
    "    | llm                             # 调用 LLM\n",
    "    | {\"English_Review\": lambda x: x}  # 将输出存储到 \"English_Review\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = (\n",
    "    {\"English_Review\": RunnablePassthrough()}  # 传递输入\n",
    "    | second_prompt                    # 应用 prompt\n",
    "    | llm                             # 调用 LLM\n",
    "    | {\"summary\": lambda x: x}  # 将输出存储到 \"English_Review\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = (\n",
    "    {\"Review\": RunnablePassthrough()}  # 传递输入\n",
    "    | third_prompt                    # 应用 prompt\n",
    "    | llm                             # 调用 LLM\n",
    "    | {\"language\": lambda x: x}  # 将输出存储到 \"English_Review\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = (\n",
    "    {\"summary\": RunnablePassthrough(), \"language\": RunnablePassthrough()}  # 传递输入\n",
    "    | fourth_prompt                    # 应用 prompt\n",
    "    | llm                             # 调用 LLM\n",
    "    | {\"followup_message\": lambda x: x}  # 将输出存储到 \"followup_message\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = (\n",
    "    {\"Review\": RunnablePassthrough()}  # 传递输入\n",
    "    | RunnableParallel(\n",
    "        {\n",
    "            \"English_Review\": chain_one,\n",
    "            \"summary\": chain_two,\n",
    "            \"language\": chain_three,\n",
    "        }\n",
    "    )\n",
    "    | RunnableParallel(\n",
    "        {\n",
    "            \"followup_message\": chain_four,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<Review>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<Review> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<Review> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<Review>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<Review>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<English_Review>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<Review>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<English_Review> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<English_Review> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<English_Review>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"English_Review\": {\n",
      "    \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"English_Review\": {\n",
      "    \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Can you summarize the following review in 1 sentence:\\n\\n{'Review': \\\"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\\\nVieux lot ou contrefaçon !?\\\"}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<Review> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<Review> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<Review>] [7ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"Review\": {\n",
      "    \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": {\n",
      "    \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Translate the following review to english:\\n\\n{'Review': \\\"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\\\nVieux lot ou contrefaçon !?\\\"}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<Review> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<Review> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<Review>] [10ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"Review\": {\n",
      "    \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": {\n",
      "    \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What language is the following review:\\n\\n{'Review': \\\"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\\\nVieux lot ou contrefaçon !?\\\"}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > llm:ChatOpenAI] [2.04s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The reviewer finds the taste mediocre, the foam lacking, and suspects it might be an old batch or counterfeit compared to the better-tasting ones they usually buy.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The reviewer finds the taste mediocre, the foam lacking, and suspects it might be an old batch or counterfeit compared to the better-tasting ones they usually buy.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 32,\n",
      "                \"prompt_tokens\": 70,\n",
      "                \"total_tokens\": 102,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"deepseek-ai/DeepSeek-V3\",\n",
      "              \"system_fingerprint\": \"\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-2acb56ca-eca1-43ac-80fa-dc53b17072b2-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 70,\n",
      "              \"output_tokens\": 32,\n",
      "              \"total_tokens\": 102,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 32,\n",
      "      \"prompt_tokens\": 70,\n",
      "      \"total_tokens\": 102,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"deepseek-ai/DeepSeek-V3\",\n",
      "    \"system_fingerprint\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<summary>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<summary> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<summary> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<summary>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence] [2.05s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > llm:ChatOpenAI] [4.84s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The review is written in **French**. Here's a translation of the review into English for better understanding:\\n\\n\\\"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\\\"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The review is written in **French**. Here's a translation of the review into English for better understanding:\\n\\n\\\"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\\\"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 57,\n",
      "                \"prompt_tokens\": 66,\n",
      "                \"total_tokens\": 123,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"deepseek-ai/DeepSeek-V3\",\n",
      "              \"system_fingerprint\": \"\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3961781e-514d-49dd-b098-e12fec73e11c-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 66,\n",
      "              \"output_tokens\": 57,\n",
      "              \"total_tokens\": 123,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 57,\n",
      "      \"prompt_tokens\": 66,\n",
      "      \"total_tokens\": 123,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"deepseek-ai/DeepSeek-V3\",\n",
      "    \"system_fingerprint\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<language>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<language> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<language> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<language>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence] [4.86s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > llm:ChatOpenAI] [10.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Here’s the English translation of the review:\\n\\n\\\"I find the taste mediocre. The foam doesn’t hold, it’s strange. I buy the same ones in stores, and the taste is much better... Old batch or counterfeit!?\\\"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Here’s the English translation of the review:\\n\\n\\\"I find the taste mediocre. The foam doesn’t hold, it’s strange. I buy the same ones in stores, and the taste is much better... Old batch or counterfeit!?\\\"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 49,\n",
      "                \"prompt_tokens\": 66,\n",
      "                \"total_tokens\": 115,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"deepseek-ai/DeepSeek-V3\",\n",
      "              \"system_fingerprint\": \"\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-fd9e7a91-8174-499e-b354-5c5af42b1920-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 66,\n",
      "              \"output_tokens\": 49,\n",
      "              \"total_tokens\": 115,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 49,\n",
      "      \"prompt_tokens\": 66,\n",
      "      \"total_tokens\": 115,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"deepseek-ai/DeepSeek-V3\",\n",
      "    \"system_fingerprint\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<English_Review>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<English_Review> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<English_Review> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence > chain:RunnableParallel<English_Review>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language> > chain:RunnableSequence] [10.65s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<English_Review,summary,language>] [10.65s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > chain:RunnableParallel<summary,language>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > chain:RunnableParallel<summary,language> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > chain:RunnableParallel<summary,language> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > chain:RunnableParallel<summary,language> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > chain:RunnableParallel<summary,language> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > chain:RunnableParallel<summary,language>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Write a follow up response to the following summary in the specified language:\\n\\nSummary: {'English_Review': {'English_Review': AIMessage(content='Here’s the English translation of the review:\\\\n\\\\n\\\"I find the taste mediocre. The foam doesn’t hold, it’s strange. I buy the same ones in stores, and the taste is much better... Old batch or counterfeit!?\\\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 66, 'total_tokens': 115, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-fd9e7a91-8174-499e-b354-5c5af42b1920-0', usage_metadata={'input_tokens': 66, 'output_tokens': 49, 'total_tokens': 115, 'input_token_details': {}, 'output_token_details': {}})}, 'summary': {'summary': AIMessage(content='The reviewer finds the taste mediocre, the foam lacking, and suspects it might be an old batch or counterfeit compared to the better-tasting ones they usually buy.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 70, 'total_tokens': 102, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-2acb56ca-eca1-43ac-80fa-dc53b17072b2-0', usage_metadata={'input_tokens': 70, 'output_tokens': 32, 'total_tokens': 102, 'input_token_details': {}, 'output_token_details': {}})}, 'language': {'language': AIMessage(content='The review is written in **French**. Here\\\\'s a translation of the review into English for better understanding:\\\\n\\\\n\\\"I find the taste mediocre. The foam doesn\\\\'t hold, it\\\\'s weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\\\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 66, 'total_tokens': 123, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-3961781e-514d-49dd-b098-e12fec73e11c-0', usage_metadata={'input_tokens': 66, 'output_tokens': 57, 'total_tokens': 123, 'input_token_details': {}, 'output_token_details': {}})}}\\n\\nLanguage: {'English_Review': {'English_Review': AIMessage(content='Here’s the English translation of the review:\\\\n\\\\n\\\"I find the taste mediocre. The foam doesn’t hold, it’s strange. I buy the same ones in stores, and the taste is much better... Old batch or counterfeit!?\\\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 66, 'total_tokens': 115, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-fd9e7a91-8174-499e-b354-5c5af42b1920-0', usage_metadata={'input_tokens': 66, 'output_tokens': 49, 'total_tokens': 115, 'input_token_details': {}, 'output_token_details': {}})}, 'summary': {'summary': AIMessage(content='The reviewer finds the taste mediocre, the foam lacking, and suspects it might be an old batch or counterfeit compared to the better-tasting ones they usually buy.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 70, 'total_tokens': 102, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-2acb56ca-eca1-43ac-80fa-dc53b17072b2-0', usage_metadata={'input_tokens': 70, 'output_tokens': 32, 'total_tokens': 102, 'input_token_details': {}, 'output_token_details': {}})}, 'language': {'language': AIMessage(content='The review is written in **French**. Here\\\\'s a translation of the review into English for better understanding:\\\\n\\\\n\\\"I find the taste mediocre. The foam doesn\\\\'t hold, it\\\\'s weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\\\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 66, 'total_tokens': 123, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-3961781e-514d-49dd-b098-e12fec73e11c-0', usage_metadata={'input_tokens': 66, 'output_tokens': 57, 'total_tokens': 123, 'input_token_details': {}, 'output_token_details': {}})}}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > llm:ChatOpenAI] [28.89s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"**Follow-up Response in French:**\\n\\nMerci pour votre commentaire détaillé. Nous sommes désolés d'apprendre que votre expérience n'a pas été à la hauteur de vos attentes. La qualité de nos produits est une priorité absolue, et il est inquiétant d'entendre que le goût et la mousse ne correspondaient pas à ce que vous avez l'habitude d'acheter en magasin. Nous prenons très au sérieux vos soupçons concernant un lot ancien ou contrefait. Afin de résoudre ce problème, nous vous invitons à nous fournir des détails supplémentaires sur le produit, tels que le numéro de lot et le lieu d'achat, afin que nous puissions enquêter et prendre les mesures appropriées. Nous tenons à vous assurer que nous ferons tout notre possible pour garantir votre satisfaction et maintenir la qualité de nos produits. N'hésitez pas à nous contacter directement pour toute assistance supplémentaire.  \\n\\nCordialement,  \\n[Votre nom/Équipe de support client]\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"**Follow-up Response in French:**\\n\\nMerci pour votre commentaire détaillé. Nous sommes désolés d'apprendre que votre expérience n'a pas été à la hauteur de vos attentes. La qualité de nos produits est une priorité absolue, et il est inquiétant d'entendre que le goût et la mousse ne correspondaient pas à ce que vous avez l'habitude d'acheter en magasin. Nous prenons très au sérieux vos soupçons concernant un lot ancien ou contrefait. Afin de résoudre ce problème, nous vous invitons à nous fournir des détails supplémentaires sur le produit, tels que le numéro de lot et le lieu d'achat, afin que nous puissions enquêter et prendre les mesures appropriées. Nous tenons à vous assurer que nous ferons tout notre possible pour garantir votre satisfaction et maintenir la qualité de nos produits. N'hésitez pas à nous contacter directement pour toute assistance supplémentaire.  \\n\\nCordialement,  \\n[Votre nom/Équipe de support client]\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 231,\n",
      "                \"prompt_tokens\": 1408,\n",
      "                \"total_tokens\": 1639,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"deepseek-ai/DeepSeek-V3\",\n",
      "              \"system_fingerprint\": \"\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e8e7e0af-050c-46ae-9641-04cf5c33bacf-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1408,\n",
      "              \"output_tokens\": 231,\n",
      "              \"total_tokens\": 1639,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 231,\n",
      "      \"prompt_tokens\": 1408,\n",
      "      \"total_tokens\": 1639,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"deepseek-ai/DeepSeek-V3\",\n",
      "    \"system_fingerprint\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > chain:RunnableParallel<followup_message>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence > chain:RunnableParallel<followup_message>] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message> > chain:RunnableSequence] [28.90s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<followup_message>] [28.91s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [39.56s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tracers import ConsoleCallbackHandler\n",
    "df = pd.read_csv('Data.csv')\n",
    "review = df.Review[5]\n",
    "response = overall_chain.invoke(review, config={\"callbacks\": [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'followup_message': {'followup_message': AIMessage(content=\"**Follow-up Response in French:**\\n\\nMerci pour votre commentaire détaillé. Nous sommes désolés d'apprendre que votre expérience n'a pas été à la hauteur de vos attentes. La qualité de nos produits est une priorité absolue, et il est inquiétant d'entendre que le goût et la mousse ne correspondaient pas à ce que vous avez l'habitude d'acheter en magasin. Nous prenons très au sérieux vos soupçons concernant un lot ancien ou contrefait. Afin de résoudre ce problème, nous vous invitons à nous fournir des détails supplémentaires sur le produit, tels que le numéro de lot et le lieu d'achat, afin que nous puissions enquêter et prendre les mesures appropriées. Nous tenons à vous assurer que nous ferons tout notre possible pour garantir votre satisfaction et maintenir la qualité de nos produits. N'hésitez pas à nous contacter directement pour toute assistance supplémentaire.  \\n\\nCordialement,  \\n[Votre nom/Équipe de support client]\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 231, 'prompt_tokens': 1408, 'total_tokens': 1639, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-V3', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-e8e7e0af-050c-46ae-9641-04cf5c33bacf-0', usage_metadata={'input_tokens': 1408, 'output_tokens': 231, 'total_tokens': 1639, 'input_token_details': {}, 'output_token_details': {}})}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(response)\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = default_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: The value of “destination” MUST match one of \\\n",
    "the candidate prompts listed below.\\\n",
    "If “destination” does not fit any of the specified prompts, set it to “DEFAULT.”\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "# 创建路由链\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=StrOutputParser(),\n",
    ")\n",
    "\n",
    "def route_input(input_data):\n",
    "    # 调用 LLM 进行路由\n",
    "    response = llm.invoke(router_prompt.format(input=input_data))\n",
    "    # 从 AIMessage 中提取 content\n",
    "    response_content = response.content\n",
    "    # 解析 JSON 输出\n",
    "    try:\n",
    "        # 去除 Markdown 代码块的标记（如 ```json 和 ```）\n",
    "        response_content = response_content.strip(\"```json\").strip(\"```\")\n",
    "        result = json.loads(response_content)\n",
    "        return result\n",
    "    except json.JSONDecodeError:\n",
    "        # 如果解析失败，返回默认值\n",
    "        return {\"destination\": \"DEFAULT\", \"next_inputs\": input_data}\n",
    "\n",
    "# 组合链\n",
    "def multi_prompt_chain(input_data):\n",
    "    routing_result = route_input(input_data)\n",
    "    destination = routing_result[\"destination\"]\n",
    "    next_inputs = routing_result[\"next_inputs\"]\n",
    "    print(f\"destination:{destination}, \\nnext_inputs:{next_inputs}\")\n",
    "    \n",
    "    if destination in destination_chains:\n",
    "        return destination_chains[destination].invoke(next_inputs)\n",
    "    else:\n",
    "        return default_chain.invoke(next_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "destination:physics, \n",
      "next_inputs:What is black body radiation?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Black body radiation is the electromagnetic radiation emitted by an idealized object called a black body, which absorbs all incident radiation and re-emits it based solely on its temperature. A black body is a perfect absorber and emitter of energy, meaning it doesn’t reflect or transmit any radiation.\\n\\nThe spectrum of black body radiation depends only on the object's temperature and follows Planck's law. As the temperature increases, the peak wavelength of the emitted radiation shifts to shorter wavelengths (Wien's displacement law), and the total energy radiated increases (Stefan-Boltzmann law). This phenomenon explains the color and intensity of light emitted by hot objects, like stars or heated metals.\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_prompt_chain(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "destination:math, \n",
      "next_inputs:what is 2 + 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To solve the question \"What is 2 + 2?\", let\\'s break it down into its component parts and then put them together to arrive at the answer.\\n\\n### Step 1: Understand the numbers\\n- The numbers involved are **2** and **2**.\\n\\n### Step 2: Understand the operation\\n- The operation is **addition (+)**. Addition means combining two or more numbers to find their total.\\n\\n### Step 3: Perform the addition\\n- Add the two numbers:  \\n  \\\\( 2 + 2 = 4 \\\\)\\n\\n### Step 4: Final Answer\\nThe result of \\\\( 2 + 2 \\\\) is **4**.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_prompt_chain(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "destination:DEFAULT, \n",
      "next_inputs:Why does every cell in our body contain DNA?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Every cell in our body contains DNA because DNA carries the essential genetic instructions necessary for the cell's growth, development, functioning, and reproduction. Here’s why DNA is present in every cell:\\n\\n1. **Genetic Blueprint**: DNA contains the genes that encode the information needed to build and maintain the organism. These genes provide the instructions for synthesizing proteins, which perform most of the functions in the cell.\\n\\n2. **Cellular Function**: Each cell, regardless of its type or function, requires specific proteins to carry out its tasks. DNA provides the code for these proteins, ensuring that each cell can perform its role effectively.\\n\\n3. **Cell Division and Reproduction**: When cells divide (through mitosis or meiosis), they need to replicate their DNA to pass on the genetic information to the daughter cells. This ensures that each new cell has the same genetic instructions as the parent cell.\\n\\n4. **Regulation of Cellular Activities**: DNA also contains regulatory sequences that control when and how genes are expressed. This regulation is crucial for the cell to respond to internal and external signals and to maintain homeostasis.\\n\\n5. **Specialization and Differentiation**: In multicellular organisms, different cells have specialized functions (e.g., muscle cells, nerve cells, blood cells). The DNA in each cell contains the same genes, but different genes are activated or repressed depending on the cell type, allowing for differentiation and specialization.\\n\\n6. **Inheritance**: DNA is the molecule of heredity. It ensures that genetic information is passed from one generation to the next, maintaining the continuity of life.\\n\\nIn summary, DNA is fundamental to the life and function of every cell because it provides the instructions necessary for the cell's activities, ensures accurate replication and inheritance, and allows for the specialization of cells in multicellular organisms.\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_prompt_chain(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
